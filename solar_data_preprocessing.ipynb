{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries and setting up file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "path=\"data/\"\n",
    "sub={'production_meter/':['average_power(kw)/'], 'weather_station/':['temperature(f)/','irradiance(w_per_m2)/']}\n",
    "med='15_minutes/'\n",
    "fin=['12_23_2019-12_31_2019', '01_01_2020-01_31_2020','02_01_2020-02_29_2020','03_01_2020-03_31_2020','04_01_2020-04_30_2020','05_01_2020-05_31_2020','06_01_2020-06_30_2020','07_01_2020-07_31_2020','08_01_2020-08_31_2020','09_01_2020-09_30_2020','10_01_2020-10_31_2020','11_01_2020-11_30_2020','12_01_2020-12_26_2020']\n",
    "end='.csv'\n",
    "\n",
    "#these will form the full path name of the raw data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs={}\n",
    "for i in sub: #going through each source for the data\n",
    "    for j in sub[i]: #going though each category in each source\n",
    "        csvs[j]=pd.DataFrame()\n",
    "        for k in fin: #going though each month available\n",
    "            t=pd.read_csv(path+i+j+med+k+end, index_col='Site Time', parse_dates=['Site Time']) #reading a full path name for one csv file, with the timestamps read as datettime objects for the index\n",
    "            csvs[j]=pd.concat([csvs[j], t])\n",
    "power, temp, irradiance=csvs['average_power(kw)/'], csvs['temperature(f)/'], csvs['irradiance(w_per_m2)/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to only keep hourly timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "power=power[power.index.minute==0] #this line is run when only hourly increments are wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating a new cyclical time feature from the timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting a cyclical time value from the timestamps\n",
    "from astral.geocoder import lookup, database\n",
    "import datetime\n",
    "city = lookup(\"Hartford\", database()) #closest city to Windsor CT, the location of the solar field\n",
    "from astral.sun import sun\n",
    "power['time']=power.index.copy() #copying the timestamp index, to be modified below\n",
    "def conversion(time): #function for converting a single timestamp\n",
    "    s = sun(city.observer, date=datetime.date(time.year, time.month, time.day))\n",
    "    sunrise=pd.to_datetime(s['sunrise'].__str__()).tz_convert('US/Eastern').tz_localize(None) #getting the sunrise time, then converting to the correct timezone and then removing timezone info\n",
    "    sunset=pd.to_datetime(s['sunset'].__str__()).tz_convert('US/Eastern').tz_localize(None) #getting the sunset time\n",
    "    noon=pd.to_datetime(s['noon'].__str__()).tz_convert('US/Eastern').tz_localize(None) #getting the noon time\n",
    "    half=(noon-sunrise).total_seconds()\n",
    "    if (time-sunrise).days<0 or (sunset-time).days<0:\n",
    "        return 0 #non daylight times will be set to 0\n",
    "    if (time-noon).days<0:\n",
    "        return 1-((noon-time).total_seconds()/half) \n",
    "    return 1-((time-noon).total_seconds()/half)\n",
    "power['time']=power['time'].apply(conversion) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing anomalous temperature values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['Weather station device temperature '][temp['Weather station device temperature ']<0]=np.nan #the anomalous values, which are less than 0 degrees f, are set to NaN\n",
    "temp.fillna(temp['Weather station device temperature '].median(), inplace=True) #the NaN values are replaced with the median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating all power data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_ahead_power=power.shift(-1).copy() #creating the target data from shifting the current power ahead one\n",
    "one_behind_power=power.shift(1).copy().drop(labels='time', axis=1) #creating a new feature from shifting current power behind one\n",
    "two_behind_power=power.shift(2).copy().drop(labels='time', axis=1) #creating a new feature from shifting current power behind two\n",
    "one_ahead_power=one_ahead_power.rename(columns={'Production meter active power Kilowatts':'one ahead power Kilowatts'})\n",
    "one_behind_power=one_behind_power.rename(columns={'Production meter active power Kilowatts':'one behind power Kilowatts'})\n",
    "two_behind_power=two_behind_power.rename(columns={'Production meter active power Kilowatts':'two behind power Kilowatts'})\n",
    "power=power.drop('time', axis=1) #the only time column now is in one_ahead_power, reflecting the time for the target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging all power data into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "power=two_behind_power.merge(one_behind_power, on='Site Time').merge(power, on='Site Time').merge(one_ahead_power, on='Site Time') #merging all the power data and aligning them along the timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows with missing values are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing missing values\n",
    "temp=temp.dropna()\n",
    "irradiance=irradiance.dropna()\n",
    "power=power.dropna()\n",
    "power=power.rename(columns={'Production meter active power Kilowatts':'current power Kilowatts'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is converted to numeric types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "power['current power Kilowatts']=pd.to_numeric(power['current power Kilowatts'].astype(str).str.replace(',','')) #the issue is that some entries have commas, like 1,000, so first they are converted to strings, then the commas are removed before conversion to numbers\n",
    "power['two behind power Kilowatts']=pd.to_numeric(power['two behind power Kilowatts'].astype(str).str.replace(',',''))\n",
    "power['one behind power Kilowatts']=pd.to_numeric(power['one behind power Kilowatts'].astype(str).str.replace(',',''))\n",
    "power['one ahead power Kilowatts']=pd.to_numeric(power['one ahead power Kilowatts'].astype(str).str.replace(',',''))\n",
    "irradiance['POA Watts/meter²'], irradiance['GHI Watts/meter²']=pd.to_numeric(irradiance['POA Watts/meter²'].astype(str).str.replace(',','')), pd.to_numeric(irradiance['GHI Watts/meter²'].astype(str).str.replace(',',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power and irradiance values less than 0 are set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "power[power<0]=0\n",
    "irradiance[irradiance<0]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging all data columns into one large DataFrame with all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_data=power.merge(temp, on='Site Time').merge(irradiance, on='Site Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line is run when only daytime values are wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_data=site_data[site_data['time']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of current large DataFrame containing all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 8218 entries, 2019-12-23 10:00:00 to 2020-12-26 22:00:00\n",
      "Data columns (total 10 columns):\n",
      " #   Column                                                  Non-Null Count  Dtype  \n",
      "---  ------                                                  --------------  -----  \n",
      " 0   two behind power Kilowatts                              8218 non-null   float64\n",
      " 1   one behind power Kilowatts                              8218 non-null   float64\n",
      " 2   current power Kilowatts                                 8218 non-null   float64\n",
      " 3   one ahead power Kilowatts                               8218 non-null   float64\n",
      " 4   time                                                    8218 non-null   float64\n",
      " 5   Weather station module temperature                      8218 non-null   float64\n",
      " 6   Weather station ambient temperature Degrees Fahrenheit  8218 non-null   float64\n",
      " 7   Weather station device temperature                      8218 non-null   float64\n",
      " 8   POA Watts/meter²                                        8218 non-null   float64\n",
      " 9   GHI Watts/meter²                                        8218 non-null   float64\n",
      "dtypes: float64(10)\n",
      "memory usage: 706.2 KB\n"
     ]
    }
   ],
   "source": [
    "site_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into a training and test set, as well as input and target sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=site_data.drop(columns=['one ahead power Kilowatts'], axis=1) #dropping the target column from input dataset\n",
    "y=site_data['one ahead power Kilowatts'].copy() #extracting the target column to form target dataset\n",
    "ratio=.8 #what percentage of the entire data the training set consists of\n",
    "X_train, X_test, y_train, y_test=X[:int(ratio*len(X))], X[int(ratio*len(X)):], y[:int(ratio*len(y))], y[int(ratio*len(y)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization of the input data by column into a fixed range -1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scaling(X_train, X_test):\n",
    "    scalers={}\n",
    "    for i in X_train.columns: #going through each column in the input\n",
    "        scaler=MinMaxScaler(feature_range=(-1,1)) #creating the scaler\n",
    "        X_train[i]=scaler.fit_transform(X_train[i].to_frame()) #converting each column\n",
    "        scalers[i]=scaler #saving each scaler with its column name\n",
    "        X_test[i]=scaler.transform(X_test[i].to_frame()) #transforming the test set column with the scaler fitted to the training set\n",
    "    return X_train, X_test, scalers\n",
    "\n",
    "X_train, X_test, xscalers=scaling(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the preprocessed datasets to csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train')\n",
    "X_test.to_csv('X_test')\n",
    "y_train.to_csv('y_train')\n",
    "y_test.to_csv('y_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serializing the scaler for the cyclical time value so that the scaling done on this column can later be reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time_transformer']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(xscalers['time'], 'time_transformer') #saving the scaler to a file called 'time_transformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
